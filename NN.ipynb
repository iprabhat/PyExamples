{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, dim):\n",
    "        self.layers = len(dim)\n",
    "        self.dimensions = dim\n",
    "        self.weight = [np.random.randn(i,j) for i,j in [(x,y) for x,y in zip(dim[1:], dim[:-1])]]\n",
    "        self.bias = [np.random.randn(y, 1) for y in dim[1:]]\n",
    "        #print(\"Weight: \",self.weight,\"\\nBias: \",self.bias)\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        \"\"\"The sigmoid function.\"\"\"\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self,z):\n",
    "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def feedforward(self,X):\n",
    "        print(\":::::: Feedforward ::::::\")\n",
    "        A = X\n",
    "        for w, b in zip(self.weight, self.bias):\n",
    "            A = self.sigmoid((np.dot(w,A)+b))\n",
    "            #print(\"A: \",A)\n",
    "            #print(\"feedforward:\\n\",\"w: \",w.shape,\"\\nb: \",b.shape)\n",
    "        return A\n",
    "    def backprop(self, X,y):\n",
    "        #print(\"::::::: Backprop :::::::\\n\")\n",
    "        #FeedForward\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weight]\n",
    "        activation = X\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for b, w in zip(self.bias, self.weight):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward pass\n",
    "        # Delta for Output Layer\n",
    "        #delta = (activations[-1] - y) * sigmoid_prime(zs[-1]) # For mean squared error\n",
    "        delta = (activations[-1] - y) # For Cross Entropy Error (Logistic Error)\n",
    "        #nabla_b[-1] = delta\n",
    "        nabla_b[-1] = (1/X.shape[1]) * np.sum(delta, axis=1, keepdims=True) #(1,1)\n",
    "        \n",
    "        #print(\"nabla_b[-1]: \",nabla_b[-1].shape,\"\\n\")\n",
    "        #print(\"activations[-1]: \",activations[-1].shape)\n",
    "        #print(\"y: \",y.shape)\n",
    "        #print(\"delta: \",delta)\n",
    "        #print(\"activations[-2]: \",activations[-2].shape)\n",
    "        #print(\"Activation Shapes:\\n\")\n",
    "        #for i in activations:\n",
    "        #    print(i.shape)\n",
    "        \n",
    "        #nabla_w[-1] =  np.dot(delta, activations[-2].transpose())        \n",
    "        nabla_w[-1] =  (1/X.shape[1]) * np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        #Delta for Hidden Layers\n",
    "        #print(\"Delta for Hidden Layers:\\n\")\n",
    "        for l in range(2, self.layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.sigmoid_prime(z)\n",
    "            \n",
    "            #print(\"\\nself.weight[-l+1].transpose():\",self.weight[-l+1].transpose().shape)\n",
    "            #print(\"\\nDelta Shape: \", delta.shape)\n",
    "            #print(\"\\nsp Shape: \",sp.shape)\n",
    "            #print(\"\\nZ: \",z.shape)\n",
    "           \n",
    "            #print(\"l: \",l,\" of \", self.layers)\n",
    "            #print(\"Weight \",l,\" : \",self.weight[-l])\n",
    "            #print(\"nabla_b[-l]: \",nabla_b[-l].shape)\n",
    "            #print(\"nabla_w[-l]: \", nabla_w[-l].shape)\n",
    "            \n",
    "            delta = np.dot(self.weight[-l+1].transpose(), delta) * sp           \n",
    "            nabla_b[-l] = (1/X.shape[1]) * np.sum(delta, axis=1, keepdims=True)\n",
    "            nabla_w[-l] = (1/X.shape[1]) * np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "            #print(\"nabla_b[-l] (After Update): \",nabla_b[-l].shape)\n",
    "            #print(\"nabla_w[-l] (After Update):\", nabla_w[-l].shape)\n",
    "            \n",
    "        #print(\"::::::: End Backprop :::::::\\n\")\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def update_weights(self,X, y,eta):\n",
    "        #print(\"::::::: Update Weights :::::::\\n\")\n",
    "        #eta = 0.01\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.bias]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weight]\n",
    "        \n",
    "        delta_nabla_b, delta_nabla_w = self.backprop(X, y)\n",
    "        #print(\"\\nnabla_b shape: \",[nb.shape for nb in nabla_b])\n",
    "        #print(\"\\nnabla_w shape: \",[nw.shape for nw in nabla_w])\n",
    "        \n",
    "        #print(\"\\ndelta_nabla_b shape: \",[nb.shape for nb in delta_nabla_b])\n",
    "        #print(\"\\ndelta_nabla_w shape: \",[nw.shape for nw in delta_nabla_w])\n",
    "                \n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        #print(\"Bias:\\n\", [b.shape for b in self.bias], \"\\nWeights:\\n\",[w.shape for w in self.weight])\n",
    "        self.weight = [w-(eta/X.shape[1])*nw for w, nw in zip(self.weight, nabla_w)]   \n",
    "        #print(\"\\nbias and nabla_b\",[(b.shape,nb.shape) for b, nb in zip(self.bias, nabla_b)])       \n",
    "        self.bias = [b-(eta/X.shape[1])*nb for b, nb in zip(self.bias, nabla_b)]\n",
    "        #print(\"Bias After Update:\\n \", [b.shape for b in self.bias],\"\\nWeights After Update:\\n\",[w.shape for w in self.weight])\n",
    "        #print(\"::::::: End Update Weights :::::::\\n\")\n",
    "    \n",
    "    def train(self, X, y, epochs=10, eta=0.01):\n",
    "        for i in range(epochs):\n",
    "            #print(\"Epoch: \",i,\"\\n\")\n",
    "            self.update_weights(X,y,eta)\n",
    "            print(\"Epoch: \",(i+1))\n",
    "    \n",
    "    def predict(self, ip):\n",
    "        return feedforward(ip)\n",
    "    \n",
    "    def display(self):\n",
    "        print(\"Weights\\n\")\n",
    "        for x in self.weight:\n",
    "            print(x,x.shape,\"\\n\")\n",
    "        print(\"Bias\\n\")\n",
    "        for y in self.bias:\n",
    "            print(y, y.shape,\"\\n\")\n",
    "    def display_2(self):\n",
    "        for b,w in zip(self.bias, self.weight):\n",
    "            print(\"Bias: \",b,b.shape,\"\\n\", \"Weight: \", w, w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Loaded\n",
      "(800, 2)\n",
      "(800, 1)\n"
     ]
    }
   ],
   "source": [
    "#Load Training Data\n",
    "import pickle\n",
    "with open(\"xor_dataset.dat\",\"rb\") as f:\n",
    "    xor_data = pickle.load(f)\n",
    "training_X = xor_data[0]\n",
    "training_Y = xor_data[1]\n",
    "print(\"Training Data Loaded\")\n",
    "print(training_X.shape)\n",
    "print(training_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2)\n"
     ]
    }
   ],
   "source": [
    "#Randomize Data\n",
    "r = np.random.rand(training_X.shape[0],2)\n",
    "training_X = training_X + r\n",
    "print(training_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train NET\n",
    "net = NN([2,2,1])\n",
    "#net.display()\n",
    "net.train(training_X.T, training_Y.T, epochs=100, eta = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::::: Feedforward ::::::\n",
      "[[ 0.1911246]]\n"
     ]
    }
   ],
   "source": [
    "#Pedict\n",
    "test_data = np.array([[0.0255,0.25]])\n",
    "#print(test_data.T.shape)\n",
    "print(net.feedforward(test_data.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cl = NN([2,3,2,1])\n",
    "cl.display_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(training_X,training_y):\n",
    "    print(\"X: \",i,\"\\ny:\",j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Sum\n",
      " (2, 1)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "print(a)\n",
    "#print(a + np.array([[5],[5]]))\n",
    "s = np.sum(a, axis=1, keepdims=True)\n",
    "print(\"Sum\\n\",s.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1.02499797, -2.23708689],\n",
      "       [-0.51879056,  0.2669871 ],\n",
      "       [-0.04187466, -0.67949619]]), array([[ 1.47762289,  0.28373874,  1.12427594]])]\n"
     ]
    }
   ],
   "source": [
    "import MyNN,numpy as np\n",
    "nn = MyNN.NN([2,3,1])\n",
    "print(nn.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
